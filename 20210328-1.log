6
segment_open.py configs/20210328-0.json
cmd : train
data_dir : datasets/populated
classes : 13
crop_size : 1024
step : 200
arch : drn_d_22
batch_size : 40
epochs : 100
lr : 0.01
lr_mode : step
momentum : 0.9
weight_decay : 0.0001
evaluate : False
resume : 
pretrained : 
workers : 8
load_rel : None
phase : val
random_scale : 0
random_rotate : 0
bn_sync : False
ms : True
with_gt : False
test_suffix : 
log_file : 20210328-0.log
DRNSeg(
  (base): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (2): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (8): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (pool1): AdaptiveAvgPool2d(output_size=1)
  (pool2): AdaptiveAvgPool2d(output_size=2)
  (pool3): AdaptiveAvgPool2d(output_size=3)
  (pool6): AdaptiveAvgPool2d(output_size=6)
  (pool1_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (pool1_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool2_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (pool2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool3_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (pool3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool6_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (pool6_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Conv2d(2560, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (fc1_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop): Dropout2d(p=0.5, inplace=False)
  (fc2): Conv2d(512, 13, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (softmax): LogSoftmax(dim=None)
  (relu): ReLU(inplace=True)
  (up): ConvTranspose2d(13, 13, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4), groups=13, bias=False)
)
/usr/aidrive1/anaconda3/envs/vl-bert/lib/python3.6/site-packages/torch/nn/modules/loss.py:219: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.
  warnings.warn("NLLLoss2d has been deprecated. "
2322
258
[2022-03-28 21:30:13,130 segment_open.py:622 train_seg] Epoch: [0]	lr 0.010000
/usr/aidrive1/anaconda3/envs/vl-bert/lib/python3.6/site-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
/usr/aidrive1/anaconda3/envs/vl-bert/lib/python3.6/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
segment_open.py:180: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return self.softmax(y), x, y, self.softmax(z)
segment_open.py:462: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  result = tempsoftmax(features)
[2022-03-28 21:30:48,370 segment_open.py:500 train] Epoch: [0][0/58]	Time 35.239 (35.239)	Data 6.468 (6.468)	Loss 7.1512 (7.1512)	Score 0.084 (0.084)
[2022-03-28 21:30:53,112 segment_open.py:500 train] Epoch: [0][1/58]	Time 4.743 (19.991)	Data 0.001 (3.234)	Loss 6.3602 (6.7557)	Score 3.926 (2.005)
[2022-03-28 21:30:55,661 segment_open.py:500 train] Epoch: [0][2/58]	Time 2.549 (14.177)	Data 0.001 (2.156)	Loss 5.9348 (6.4821)	Score 7.198 (3.736)
[2022-03-28 21:30:58,377 segment_open.py:500 train] Epoch: [0][3/58]	Time 2.716 (11.312)	Data 0.001 (1.617)	Loss 5.7812 (6.3068)	Score 9.727 (5.234)
[2022-03-28 21:31:01,078 segment_open.py:500 train] Epoch: [0][4/58]	Time 2.701 (9.589)	Data 0.000 (1.294)	Loss 5.5034 (6.1462)	Score 12.042 (6.595)
[2022-03-28 21:31:03,798 segment_open.py:500 train] Epoch: [0][5/58]	Time 2.719 (8.444)	Data 0.001 (1.078)	Loss 5.0595 (5.9650)	Score 21.818 (9.132)
[2022-03-28 21:31:06,652 segment_open.py:500 train] Epoch: [0][6/58]	Time 2.855 (7.646)	Data 0.001 (0.924)	Loss 5.1575 (5.8497)	Score 22.548 (11.049)
[2022-03-28 21:31:09,386 segment_open.py:500 train] Epoch: [0][7/58]	Time 2.733 (7.032)	Data 0.001 (0.809)	Loss 4.9129 (5.7326)	Score 30.719 (13.508)
[2022-03-28 21:31:12,117 segment_open.py:500 train] Epoch: [0][8/58]	Time 2.732 (6.554)	Data 0.001 (0.719)	Loss 4.6797 (5.6156)	Score 37.012 (16.119)
[2022-03-28 21:31:14,839 segment_open.py:500 train] Epoch: [0][9/58]	Time 2.722 (6.171)	Data 0.001 (0.647)	Loss 4.8338 (5.5374)	Score 34.221 (17.929)
[2022-03-28 21:31:17,562 segment_open.py:500 train] Epoch: [0][10/58]	Time 2.723 (5.857)	Data 0.001 (0.589)	Loss 4.7117 (5.4624)	Score 36.259 (19.596)
[2022-03-28 21:31:20,283 segment_open.py:500 train] Epoch: [0][11/58]	Time 2.722 (5.596)	Data 0.001 (0.540)	Loss 4.4869 (5.3811)	Score 39.879 (21.286)
[2022-03-28 21:31:23,013 segment_open.py:500 train] Epoch: [0][12/58]	Time 2.730 (5.376)	Data 0.001 (0.498)	Loss 4.7695 (5.3340)	Score 33.121 (22.196)
